{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Deep Convolutional Q-Learning Pacman",
   "id": "306fb1d32eb9891b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:03:37.133515Z",
     "start_time": "2024-08-12T18:03:36.140964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Build an AI\n",
    "![Description](https://gymnasium.farama.org/_images/pacman.gif)"
   ],
   "id": "7c81c84594793f69"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:03:39.190812Z",
     "start_time": "2024-08-12T18:03:39.182567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, action_size, seed = 42):\n",
    "        super(Network, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.fc1 = nn.Linear(128 * 10 * 10, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    "
   ],
   "id": "c5772893255ab9bf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training the AI",
   "id": "fab689294badddc2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:03:41.216406Z",
     "start_time": "2024-08-12T18:03:40.902031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('MsPacmanDeterministic-v4', full_action_space=False)\n",
    "state_shape = env.observation_space.shape\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "print('State shape:', state_shape)\n",
    "print('State size:', state_size)\n",
    "print('Action size:', action_size)\n",
    "print(\"Name of the actions: \", env.unwrapped.get_action_meanings())"
   ],
   "id": "1bed5ccc46e7db4c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape: (210, 160, 3)\n",
      "State size: 210\n",
      "Action size: 9\n",
      "Name of the actions:  ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:04:04.791126Z",
     "start_time": "2024-08-12T18:04:04.787268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initializing the hyperparameters\n",
    "learning_rate = 5e-4\n",
    "minibatch_size = 64\n",
    "discount_factor = 0.99"
   ],
   "id": "bdfdfed3d8118782",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:04:06.344279Z",
     "start_time": "2024-08-12T18:04:05.576664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Processing the frames to make them suitable for the network\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "def process_frame(frame):\n",
    "    frame = Image.fromarray(frame)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    return preprocess(frame).unsqueeze(0)"
   ],
   "id": "5bf74516346791b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:04:07.083250Z",
     "start_time": "2024-08-12T18:04:06.982544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Implementing the DCQN algorithm\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self, action_size, seed = 42):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.local_qnetwork = Network( action_size).to(self.device)\n",
    "        self.target_qnetwork = Network( action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr=learning_rate)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        state = process_frame(state)\n",
    "        next_state = process_frame(next_state)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if len(self.memory) > minibatch_size:\n",
    "            experiences = random.sample(self.memory, k=minibatch_size)\n",
    "            self.learn(experiences, discount_factor)\n",
    "    \n",
    "    def act(self, state, epsilon = 0.01):\n",
    "        state = process_frame(state).to(self.device)\n",
    "        self.local_qnetwork.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.local_qnetwork(state)\n",
    "        self.local_qnetwork.train()\n",
    "        if random.random() > epsilon:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "    \n",
    "    def learn(self, experiences, discount_factor):\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        states = torch.from_numpy(np.vstack(states)).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack(actions)).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack(next_states)).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(self.device)\n",
    "        next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        q_targets = rewards + (discount_factor * next_q_targets * (1 - dones))\n",
    "        q_expected = self.local_qnetwork(states).gather(1, actions)\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ],
   "id": "379aee648b96c901",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:04:07.935704Z",
     "start_time": "2024-08-12T18:04:07.850123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initializing the DCQN agent\n",
    "agent = Agent(action_size)"
   ],
   "id": "f76d236c925dd619",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T21:16:12.453191Z",
     "start_time": "2024-08-08T01:16:29.862102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training the agent\n",
    "number_episodes = 2000\n",
    "maximum_number_time_steps_per_episode = 10000\n",
    "epsilon_starting_value = 1.0\n",
    "epsilon_end_value = 0.01\n",
    "epsilon_decay_rate = 0.995\n",
    "epsilon = epsilon_starting_value\n",
    "scores_on_100_episodes = deque(maxlen=100)\n",
    "\n",
    "for episode in range(1, number_episodes+1):\n",
    "    state, _ = env.reset()\n",
    "    score = 0\n",
    "    for time_step in range(maximum_number_time_steps_per_episode):\n",
    "        action = agent.act(state, epsilon)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    scores_on_100_episodes.append(score)\n",
    "    epsilon = max(epsilon_end_value, epsilon_decay_rate*epsilon)\n",
    "\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)), end=\"\")\n",
    "    if episode % 100 == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)))\n",
    "\n",
    "    if np.mean(scores_on_100_episodes) >= 500.0:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode-100, np.mean(scores_on_100_episodes)))\n",
    "        torch.save(agent.local_qnetwork.state_dict(), 'pacman_deep_convolutional_q_learning.pth')\n",
    "        break"
   ],
   "id": "e5998ca2b3c0a590",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 274.50\n",
      "Episode 200\tAverage Score: 375.50\n",
      "Episode 300\tAverage Score: 416.50\n",
      "Episode 400\tAverage Score: 460.30\n",
      "Episode 482\tAverage Score: 501.30\n",
      "Environment solved in 382 episodes!\tAverage Score: 501.30\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:04:56.268838Z",
     "start_time": "2024-08-12T18:04:51.989007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import imageio\n",
    "from datetime import datetime\n",
    "# from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "# Load the model with the best performance\n",
    "agent.local_qnetwork.load_state_dict(torch.load('pacman_deep_convolutional_q_learning.pth', weights_only=False))\n",
    "\n",
    "# Initialize the environment\n",
    "def show_video_of_model(agent, env_name):\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    frames = []\n",
    "    while not done:\n",
    "        frame = env.render()\n",
    "        frame = frame[:frame.shape[0] - frame.shape[0] % 16, :frame.shape[1] - frame.shape[1] % 16]\n",
    "        frames.append(frame)\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "    env.close()\n",
    "    \n",
    "    now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "    video_filename = f'video_{env_name}_{now}.mp4'\n",
    "    \n",
    "    imageio.mimsave(video_filename, frames, fps=30, quality=10)\n",
    "\n",
    "show_video_of_model(agent, 'MsPacman-v4')"
   ],
   "id": "e3c19b298db739e1",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "126c1df8ed8983de"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
