{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Kung_Fu Asynchronous Advantage Actor-Critic (A3C)",
   "id": "e72d0b758614fa6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:35:19.212678Z",
     "start_time": "2024-08-12T18:35:17.970015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "from gymnasium import ObservationWrapper\n",
    "from gymnasium.spaces import Box"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creating the architecture of the Neural Network\n",
    "![Description](https://gymnasium.farama.org/_images/kung_fu_master.gif)"
   ],
   "id": "5afac3a9a426f546"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:35:19.217542Z",
     "start_time": "2024-08-12T18:35:19.213512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, action_size):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels = 4,  out_channels = 32, kernel_size = (3,3), stride = 2)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = (3,3), stride = 2)\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = (3,3), stride = 2)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc1  = torch.nn.Linear(512, 128)\n",
    "        self.fc2a = torch.nn.Linear(128, action_size)\n",
    "        self.fc2s = torch.nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.conv1(state)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        action_values = self.fc2a(x)\n",
    "        state_value = self.fc2s(x).squeeze(-1)\n",
    "        return action_values, state_value"
   ],
   "id": "a030ca69b674dbee",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting up the environment",
   "id": "b4be261dd7db08c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:35:20.120246Z",
     "start_time": "2024-08-12T18:35:19.967527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PreprocessAtari(ObservationWrapper):\n",
    "\n",
    "    def __init__(self, env, height = 42, width = 42, crop = lambda img: img, dim_order = 'pytorch', color = False, n_frames = 4):\n",
    "        super(PreprocessAtari, self).__init__(env)\n",
    "        self.img_size = (height, width)\n",
    "        self.crop = crop\n",
    "        self.dim_order = dim_order\n",
    "        self.color = color\n",
    "        self.frame_stack = n_frames\n",
    "        n_channels = 3 * n_frames if color else n_frames\n",
    "        obs_shape = {'tensorflow': (height, width, n_channels), 'pytorch': (n_channels, height, width)}[dim_order]\n",
    "        self.observation_space = Box(0.0, 1.0, obs_shape)\n",
    "        self.frames = np.zeros(obs_shape, dtype = np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.frames = np.zeros_like(self.frames)\n",
    "        obs, info = self.env.reset()\n",
    "        self.update_buffer(obs)\n",
    "        return self.frames, info\n",
    "\n",
    "    def observation(self, img):\n",
    "        img = self.crop(img)\n",
    "        img = cv2.resize(img, self.img_size)\n",
    "        if not self.color:\n",
    "            if len(img.shape) == 3 and img.shape[2] == 3:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = img.astype('float32') / 255.\n",
    "        if self.color:\n",
    "            self.frames = np.roll(self.frames, shift = -3, axis = 0)\n",
    "        else:\n",
    "            self.frames = np.roll(self.frames, shift = -1, axis = 0)\n",
    "        if self.color:\n",
    "            self.frames[-3:] = img\n",
    "        else:\n",
    "            self.frames[-1] = img\n",
    "        return self.frames\n",
    "\n",
    "    def update_buffer(self, obs):\n",
    "        self.frames = self.observation(obs)\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"ALE/KungFuMaster-v5\", render_mode = 'rgb_array')\n",
    "    env = PreprocessAtari(env, height = 42, width = 42, crop = lambda img: img, dim_order = 'pytorch', color = False, n_frames = 4)\n",
    "    return env\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "state_shape = env.observation_space.shape\n",
    "number_actions = env.action_space.n\n",
    "print(\"State shape:\", state_shape)\n",
    "print(\"Number actions:\", number_actions)\n",
    "print(\"Action names:\", env.unwrapped.get_action_meanings())"
   ],
   "id": "b74b6bf63fb85d28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape: (4, 42, 42)\n",
      "Number actions: 14\n",
      "Action names: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:35:21.250462Z",
     "start_time": "2024-08-12T18:35:21.247738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learning_rate = 1e-4\n",
    "discount_factor = 0.99\n",
    "number_environments = 32"
   ],
   "id": "2073e9bd08d6b486",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Implementing the A3C model",
   "id": "67b7c0c8d7457952"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:35:23.873454Z",
     "start_time": "2024-08-12T18:35:23.861159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, action_size):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.action_size = action_size\n",
    "        self.network = Network(action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr = learning_rate)\n",
    "\n",
    "    def act(self, state):\n",
    "        if state.ndim == 3:\n",
    "            state = [state]\n",
    "        state = torch.tensor(state, dtype = torch.float32, device=self.device)\n",
    "        action_values, _ = self.network(state)\n",
    "        policy = F.softmax(action_values, dim = -1)\n",
    "        return np.array([np.random.choice(len(p), p = p) for p in policy.cpu().detach().numpy()])\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        batch_size = state.shape[0]\n",
    "\n",
    "        # Convert states to numpy arrays if they are lists of arrays\n",
    "        if isinstance(state, list):\n",
    "            state = np.array(state)\n",
    "        if isinstance(next_state, list):\n",
    "            next_state = np.array(next_state)\n",
    "        \n",
    "        state = torch.tensor(state, dtype = torch.float32, device=self.device)\n",
    "        next_state = torch.tensor(next_state, dtype = torch.float32, device=self.device)\n",
    "        reward = torch.tensor(reward, dtype = torch.float32, device=self.device)\n",
    "        done = torch.tensor(done, dtype = torch.bool, device=self.device).to(dtype=torch.float32)\n",
    "        action_values, state_values = self.network(state)\n",
    "        _, next_state_values = self.network(next_state)\n",
    "        target_state_values = reward + discount_factor * next_state_values * (1 - done)\n",
    "\n",
    "        # Ensure the shapes match\n",
    "        target_state_values = target_state_values.view(-1)\n",
    "        state_values = state_values.view(-1)\n",
    "        \n",
    "        advantage = target_state_values - state_values\n",
    "        probs = F.softmax(action_values, dim = -1)\n",
    "        logprobs = F.log_softmax(action_values, dim = -1)\n",
    "        entropy = -torch.sum(probs * logprobs, dim = -1)\n",
    "        batch_idx = np.arange(batch_size)\n",
    "        logp_actions = logprobs[batch_idx, action]\n",
    "        actor_loss = -(logp_actions * advantage.detach()).mean() - 0.001 * entropy.mean()\n",
    "        critic_loss = F.mse_loss(target_state_values.detach(), state_values)\n",
    "        total_loss = actor_loss + critic_loss\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()"
   ],
   "id": "e5528a144c522cc4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:35:25.924113Z",
     "start_time": "2024-08-12T18:35:25.506264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the agent\n",
    "agent = Agent(number_actions)"
   ],
   "id": "7c854468adf0fd7",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:35:34.983209Z",
     "start_time": "2024-08-12T18:35:34.978356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate the agent\n",
    "def evaluate(agent, env, n_episodes = 1):\n",
    "    episodes_rewards = []\n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info, _ = env.step(action[0])\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        episodes_rewards.append(total_reward)\n",
    "    return episodes_rewards"
   ],
   "id": "721b7cb21e01682f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:35:36.325026Z",
     "start_time": "2024-08-12T18:35:36.319186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Managing multiple environments\n",
    "class EnvBatch:\n",
    "    \n",
    "    def __init__(self, n_envs = 16):\n",
    "        self.envs = [make_env() for _ in range(n_envs)]\n",
    "        \n",
    "    def reset(self):\n",
    "        _states = []\n",
    "        for env in self.envs:\n",
    "            _states.append(env.reset()[0])\n",
    "        return np.array(_states)\n",
    "\n",
    "    def step(self, actions):\n",
    "        next_states, rewards, dones, infos, _ = map(np.array, zip(*[env.step(a) for env, a in zip(self.envs, actions)]))\n",
    "        for i in range(len(self.envs)):\n",
    "            if dones[i]:\n",
    "                next_states[i] = self.envs[i].reset()[0]\n",
    "        return next_states, rewards, dones, infos"
   ],
   "id": "c1f117dcfb152663",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training the agent",
   "id": "88a8fdab2ac54d67"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T20:16:39.060860Z",
     "start_time": "2024-08-09T17:28:38.060750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tqdm\n",
    "import os\n",
    "import imageio\n",
    "\n",
    "# Directory to store the videos\n",
    "video_dir = 'kung_fu_training_videos'\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "env_batch = EnvBatch(number_environments)\n",
    "batch_states = env_batch.reset()\n",
    "\n",
    "total_number_iterations = 300000\n",
    "\n",
    "with tqdm.trange(0, total_number_iterations) as progress_bar:\n",
    "    for i in progress_bar:\n",
    "        batch_actions = agent.act(batch_states)\n",
    "        batch_next_states, batch_rewards, batch_dones, _ = env_batch.step(batch_actions)\n",
    "        batch_rewards *= 0.01\n",
    "        agent.step(batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones)\n",
    "        batch_states = batch_next_states\n",
    "        # Capture video for the first 5 iterations and every 500th iteration\n",
    "        if i in [0, 1000, 5000] or i % 20000 == 0:\n",
    "            episode_frames = []\n",
    "            state, _ = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                frame = env.render()\n",
    "                frame = frame[:frame.shape[0] - frame.shape[0] % 16, :frame.shape[1] - frame.shape[1] % 16]\n",
    "                episode_frames.append(frame)\n",
    "                action = agent.act(np.expand_dims(state, axis=0))[0] \n",
    "                state, reward, done, _, _ = env.step(action)\n",
    "            env.close()\n",
    "\n",
    "            # Save video\n",
    "            video_filename = f'kung_fu_training_video_{i}.mp4'\n",
    "            video_file_loc = os.path.join(video_dir, video_filename)\n",
    "            imageio.mimsave(video_file_loc, episode_frames, fps=30)\n",
    "\n",
    "            print(\"Saved video for iteration:\", i)\n",
    "            print(\"Average agent reward:\", np.mean(evaluate(agent, env, n_episodes=16)))\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    # Save the model\n",
    "    torch.save(agent.network.state_dict(), f'kung_fu_agent_number_training_{total_number_iterations}.pth')"
   ],
   "id": "723e7b7b43c8c969",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video for iteration: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/300000 [00:28<357:34:34,  4.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average agent reward: 525.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 998/300000 [01:01<2:32:16, 32.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video for iteration: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1005/300000 [01:32<145:29:20,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average agent reward: 831.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4998/300000 [03:36<2:32:58, 32.14it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video for iteration: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 5005/300000 [04:03<123:15:32,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average agent reward: 375.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 19999/300000 [11:46<2:17:43, 33.88it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video for iteration: 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20005/300000 [12:16<141:37:12,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average agent reward: 725.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 39998/300000 [22:59<2:09:37, 33.43it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video for iteration: 40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 40005/300000 [23:49<202:22:23,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average agent reward: 3918.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 60000/300000 [33:38<2:03:50, 32.30it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video for iteration: 60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 60005/300000 [34:15<157:31:01,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average agent reward: 1687.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 79997/300000 [44:27<1:48:11, 33.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video for iteration: 80000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 80005/300000 [45:02<113:32:25,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average agent reward: 2725.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 99997/300000 [54:58<1:34:35, 35.24it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video for iteration: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 100005/300000 [55:26<82:01:50,  1.48s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average agent reward: 1462.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 120000/300000 [1:05:27<1:26:29, 34.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video for iteration: 120000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 120005/300000 [1:05:50<72:34:05,  1.45s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average agent reward: 118.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 139998/300000 [1:16:26<1:21:58, 32.53it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video for iteration: 140000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 140005/300000 [1:16:49<55:20:56,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average agent reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 159998/300000 [1:27:44<1:23:19, 28.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video for iteration: 160000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 160005/300000 [1:28:06<51:20:01,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average agent reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 179998/300000 [1:39:32<1:15:25, 26.52it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video for iteration: 180000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 180005/300000 [1:39:56<51:01:51,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average agent reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 199999/300000 [1:50:58<55:07, 30.23it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video for iteration: 200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 200003/300000 [1:51:21<45:54:00,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average agent reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 220000/300000 [2:02:11<1:00:35, 22.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video for iteration: 220000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 220005/300000 [2:02:35<36:30:34,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average agent reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 239999/300000 [2:13:24<32:59, 30.32it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video for iteration: 240000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 240005/300000 [2:13:45<21:01:32,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average agent reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 259999/300000 [2:24:49<20:53, 31.92it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video for iteration: 260000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 260004/300000 [2:25:14<18:12:30,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average agent reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 279997/300000 [2:36:26<10:40, 31.25it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video for iteration: 280000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 280005/300000 [2:36:49<6:43:08,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average agent reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [2:47:58<00:00, 29.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:42:13.623689Z",
     "start_time": "2024-08-12T18:42:11.250398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import imageio\n",
    "from datetime import datetime\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Load the model with the best performance\n",
    "agent.network.load_state_dict(torch.load('kung_fu_agent_number_training_300000.pth', weights_only=False))\n",
    "\n",
    "# Initialize the environment\n",
    "def show_video_of_model(agent, env_name):\n",
    "    # env = gym.make(env_name, render_mode='rgb_array')\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    frames = []\n",
    "    while not done:\n",
    "        frame = env.render()\n",
    "        frame = frame[:frame.shape[0] - frame.shape[0] % 16, :frame.shape[1] - frame.shape[1] % 16]\n",
    "        frames.append(frame)\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, _, _ = env.step(action[0])\n",
    "    env.close()\n",
    "    \n",
    "    now = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n",
    "    video_filename = f'video_kungfu_master_{now}.mp4'\n",
    "    \n",
    "    imageio.mimsave(video_filename, frames, fps=30, quality=10)\n",
    "    \n",
    "# Display the video\n",
    "env = make_env()\n",
    "show_video_of_model(agent, env)"
   ],
   "id": "baa4666704ca83e3",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ceb9dac0cd37932d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
